{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b6b17a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf0da61",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "from datasets import load_dataset\n",
    "from torchmetrics import StructuralSimilarityIndexMeasure\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e21cfb0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "def load_div2k_dataset_manual(\n",
    "    train_hr_dir='/content/drive/MyDrive/BSDS500/images/train',\n",
    "    valid_hr_dir='/content/drive/MyDrive/BSDS500/images/val',\n",
    "    num_samples=200\n",
    "):\n",
    "    \"\"\"\n",
    "    Manually load DIV2K HR image pairs from local directories by creating synthetic LR images.\n",
    "\n",
    "    Parameters:\n",
    "    - train_hr_dir: Path to DIV2K_train_HR\n",
    "    - valid_hr_dir: Path to DIV2K_valid_HR\n",
    "    - num_samples: Total samples to use (split 80/20 between train and val)\n",
    "    \"\"\"\n",
    "\n",
    "    hr_tf = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    lr_tf = transforms.Compose([\n",
    "        transforms.Resize((128, 128), interpolation=Image.BICUBIC),\n",
    "        transforms.Resize((256, 256), interpolation=Image.BICUBIC),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    train_pairs = []\n",
    "    val_pairs = []\n",
    "\n",
    "    # Process training images\n",
    "    train_filenames = sorted(os.listdir(train_hr_dir))[:int(0.8 * num_samples)]\n",
    "    for fname in train_filenames:\n",
    "        hr_path = os.path.join(train_hr_dir, fname)\n",
    "        try:\n",
    "            hr_img = Image.open(hr_path).convert(\"RGB\")\n",
    "            hr_tensor = hr_tf(hr_img)\n",
    "            lr_tensor = lr_tf(hr_img)\n",
    "\n",
    "            train_pairs.append((lr_tensor, hr_tensor))\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading training image {fname}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Process validation images\n",
    "    val_filenames = sorted(os.listdir(valid_hr_dir))[:num_samples - len(train_pairs)]\n",
    "    for fname in val_filenames:\n",
    "        hr_path = os.path.join(valid_hr_dir, fname)\n",
    "        try:\n",
    "            hr_img = Image.open(hr_path).convert(\"RGB\")\n",
    "            hr_tensor = hr_tf(hr_img)\n",
    "            lr_tensor = lr_tf(hr_img)\n",
    "\n",
    "            val_pairs.append((lr_tensor, hr_tensor))\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading validation image {fname}: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"Loaded {len(train_pairs)} training pairs and {len(val_pairs)} validation pairs\")\n",
    "    print(f\"Train HR path: {train_hr_dir}\")\n",
    "    print(f\"Valid HR path: {valid_hr_dir}\")\n",
    "    return train_pairs, val_pairs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c6af14",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class TeacherSharpeningModel(nn.Module):\n",
    "    \"\"\"Teacher model based on ResNet50 encoder-decoder\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Use ResNet50 with default weights\n",
    "        base = models.resnet50(weights='DEFAULT')\n",
    "        # Remove the last two layers (avgpool and fc) to keep spatial dimensions\n",
    "        # ResNet50 encoder output has 2048 channels before avgpool\n",
    "        self.encoder = nn.Sequential(*list(base.children())[:-2])\n",
    "\n",
    "        # Add adaptive pooling to ensure consistent size\n",
    "        # ResNet50 encoder output spatial size is 8x8 for 256x256 input\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((8, 8))\n",
    "\n",
    "        # Decoder needs to handle 2048 input channels from ResNet50 encoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            # From 8x8 to 16x16\n",
    "            nn.ConvTranspose2d(2048, 1024, 4, stride=2, padding=1), # Adjusted input channels\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # From 16x16 to 32x32\n",
    "            nn.ConvTranspose2d(1024, 512, 4, stride=2, padding=1), # Adjusted channels\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # From 32x32 to 64x64\n",
    "            nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1), # Adjusted channels\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # From 64x64 to 128x128\n",
    "            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1), # Adjusted channels\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # From 128x128 to 256x256\n",
    "            nn.ConvTranspose2d(128, 3, 4, stride=2, padding=1), # Adjusted channels\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Debug: print input shape\n",
    "        # print(f\"Input shape: {x.shape}\")\n",
    "\n",
    "        x = self.encoder(x)\n",
    "        # print(f\"After encoder: {x.shape}\")\n",
    "\n",
    "        x = self.adaptive_pool(x)\n",
    "        # print(f\"After adaptive pool: {x.shape}\")\n",
    "\n",
    "        x = self.decoder(x)\n",
    "        # print(f\"Output shape: {x.shape}\")\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9101e57c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class StudentSharpeningModel(nn.Module):\n",
    "    \"\"\"Enhanced student model with skip connections and Tanh output\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),  # Downsample\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.up = nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1)  # Upsample\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.Conv2d(32, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 3, 3, padding=1),\n",
    "            nn.Tanh()  # Tanh for sharper output\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.enc1(x)   # (B, 32, H, W)\n",
    "        x2 = self.enc2(x1)  # (B, 64, H/2, W/2)\n",
    "\n",
    "        up = self.up(x2)    # (B, 32, H, W)\n",
    "        up = up + x1        # Skip connection\n",
    "\n",
    "        out = self.dec(up)  # (B, 3, H, W)\n",
    "        return (out + 1) / 2  # Rescale from [-1, 1] to [0, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89e9781",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def train_student_model(train_pairs, val_pairs, epochs=5):\n",
    "    \"\"\"Train student model using knowledge distillation\"\"\"\n",
    "\n",
    "    # Initialize models\n",
    "    teacher_model = TeacherSharpeningModel().to(device)\n",
    "    student_model = StudentSharpeningModel().to(device)\n",
    "\n",
    "    # Set teacher to eval mode (no training)\n",
    "    teacher_model.eval()\n",
    "\n",
    "    # Convert pairs to tensors\n",
    "    lr_imgs = torch.stack([x[0] for x in train_pairs])\n",
    "    hr_imgs = torch.stack([x[1] for x in train_pairs])\n",
    "\n",
    "    val_lr_imgs = torch.stack([x[0] for x in val_pairs])\n",
    "    val_hr_imgs = torch.stack([x[1] for x in val_pairs])\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(lr_imgs, hr_imgs), batch_size=8, shuffle=True)\n",
    "\n",
    "    # Loss functions and optimizer\n",
    "    mse_loss = nn.MSELoss()\n",
    "    ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n",
    "    optimizer = torch.optim.Adam(student_model.parameters(), lr=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        student_model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        for lr, hr in progress_bar:\n",
    "            lr = lr.to(device)\n",
    "            hr = hr.to(device)\n",
    "\n",
    "            # Generate teacher output (no gradients)\n",
    "            with torch.no_grad():\n",
    "                teacher_output = teacher_model(lr)\n",
    "\n",
    "            # Student output\n",
    "            student_output = student_model(lr)\n",
    "\n",
    "            # Combined loss: distillation + ground truth\n",
    "            loss_distill = mse_loss(student_output, teacher_output)\n",
    "            loss_gt = mse_loss(student_output, hr)\n",
    "            loss_ssim = 1 - ssim_metric(student_output, hr)\n",
    "\n",
    "            # Weighted combination\n",
    "            total_loss = 0.4 * loss_distill + 0.4 * loss_gt + 0.2 * loss_ssim\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += total_loss.item()\n",
    "            progress_bar.set_postfix({'loss': f'{total_loss.item():.4f}'})\n",
    "\n",
    "        scheduler.step()\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] - Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        if epoch % 2 == 0:\n",
    "            val_ssim = evaluate_model(student_model, val_lr_imgs, val_hr_imgs)\n",
    "            print(f\"Validation SSIM: {val_ssim:.4f}\")\n",
    "\n",
    "    return student_model, teacher_model, val_lr_imgs, val_hr_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f7e7b3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_lr, val_hr):\n",
    "    \"\"\"Evaluate model performance\"\"\"\n",
    "    model.eval()\n",
    "    ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_lr = val_lr.to(device)\n",
    "        val_hr = val_hr.to(device)\n",
    "        pred = model(val_lr)\n",
    "        ssim_score = ssim_metric(pred, val_hr).item()\n",
    "\n",
    "    return ssim_score\n",
    "\n",
    "def calculate_psnr(img1, img2):\n",
    "    \"\"\"Calculate PSNR between two images\"\"\"\n",
    "    mse = torch.mean((img1 - img2) ** 2)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    return 20 * torch.log10(1.0 / torch.sqrt(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0daf8e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def show_results(student_model, val_lr, val_hr, index=0):\n",
    "    \"\"\"Display comparison results\"\"\"\n",
    "    student_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        lr_img = val_lr[index].cpu().permute(1, 2, 0)\n",
    "        hr_img = val_hr[index].cpu().permute(1, 2, 0)\n",
    "        pred_img = student_model(val_lr[index:index+1].to(device)).squeeze(0).cpu().permute(1, 2, 0)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    axs[0].imshow(lr_img.clamp(0, 1))\n",
    "    axs[0].set_title('Low Resolution Input')\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    axs[1].imshow(pred_img.clamp(0, 1))\n",
    "    axs[1].set_title('Student Model Output')\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    axs[2].imshow(hr_img.clamp(0, 1))\n",
    "    axs[2].set_title('Ground Truth HR')\n",
    "    axs[2].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate metrics\n",
    "    psnr = calculate_psnr(pred_img, hr_img)\n",
    "    print(f\"PSNR: {psnr:.2f} dB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cca8e6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def benchmark_fps(model, resolution=(1080, 1920)):\n",
    "    \"\"\"Benchmark model FPS at given resolution\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Create test input\n",
    "    test_input = torch.randn(1, 3, resolution[0], resolution[1]).to(device)\n",
    "\n",
    "    # Warmup\n",
    "    with torch.no_grad():\n",
    "        for _ in range(5):\n",
    "            _ = model(test_input)\n",
    "\n",
    "    # Benchmark\n",
    "    torch.cuda.synchronize() if device == 'cuda' else None\n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(30):\n",
    "            _ = model(test_input)\n",
    "\n",
    "    torch.cuda.synchronize() if device == 'cuda' else None\n",
    "    end_time = time.time()\n",
    "\n",
    "    fps = 30 / (end_time - start_time)\n",
    "    print(f\"FPS at {resolution[0]}x{resolution[1]}: {fps:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff09daa1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    # Load dataset\n",
    "    train_pairs, val_pairs = load_div2k_dataset_manual(\n",
    "        train_hr_dir='/content/drive/MyDrive/BSDS500/images/train',\n",
    "        valid_hr_dir='/content/drive/MyDrive/BSDS500/images/val',\n",
    "        num_samples=200\n",
    "    )\n",
    "\n",
    "    if len(train_pairs) == 0 or len(val_pairs) == 0:\n",
    "        print(\"Error: No data loaded. Please check dataset loading.\")\n",
    "        return\n",
    "\n",
    "\n",
    "    # Initialize models\n",
    "    print(\"\\nInitializing Teacher and Student models...\")\n",
    "    # Use the updated Teacher model with ResNet50\n",
    "    teacher_model = TeacherSharpeningModel().to(device)\n",
    "    student_model = StudentSharpeningModel().to(device)\n",
    "\n",
    "    # Set teacher to eval mode (no training)\n",
    "    teacher_model.eval()\n",
    "\n",
    "    # Convert pairs to tensors\n",
    "    lr_imgs = torch.stack([x[0] for x in train_pairs])\n",
    "    hr_imgs = torch.stack([x[1] for x in train_pairs])\n",
    "\n",
    "    val_lr_imgs = torch.stack([x[0] for x in val_pairs])\n",
    "    val_hr_imgs = torch.stack([x[1] for x in val_pairs])\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(lr_imgs, hr_imgs), batch_size=8, shuffle=True)\n",
    "\n",
    "    # Loss functions and optimizer for the Student model\n",
    "    mse_loss = nn.MSELoss()\n",
    "    ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n",
    "    optimizer = torch.optim.Adam(student_model.parameters(), lr=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
    "\n",
    "    epochs = 25 # You can adjust the number of epochs\n",
    "\n",
    "    # Training loop for the Student model with Knowledge Distillation\n",
    "    print(\"\\nStarting training with Student model and Knowledge Distillation...\")\n",
    "    for epoch in range(epochs):\n",
    "        student_model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        for lr, hr in progress_bar:\n",
    "            lr = lr.to(device)\n",
    "            hr = hr.to(device)\n",
    "\n",
    "            # Generate teacher output (no gradients)\n",
    "            with torch.no_grad():\n",
    "                teacher_output = teacher_model(lr)\n",
    "\n",
    "            # Student output\n",
    "            student_output = student_model(lr)\n",
    "\n",
    "            # Combined loss: distillation + ground truth + SSIM\n",
    "            loss_distill = mse_loss(student_output, teacher_output) # Loss against Teacher output\n",
    "            loss_gt = mse_loss(student_output, hr) # Loss against Ground Truth\n",
    "            loss_ssim = 1 - ssim_metric(student_output, hr) # SSIM loss\n",
    "\n",
    "            # Weighted combination (adjust weights as needed)\n",
    "            # Example weights: 40% distillation, 40% ground truth, 20% SSIM\n",
    "            total_loss = 0.4 * loss_distill + 0.4 * loss_gt + 0.2 * loss_ssim\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += total_loss.item()\n",
    "            progress_bar.set_postfix({'loss': f'{total_loss.item():.4f}'})\n",
    "\n",
    "        scheduler.step()\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] - Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        if epoch % 2 == 0:\n",
    "            val_ssim = evaluate_model(student_model, val_lr_imgs, val_hr_imgs)\n",
    "            print(f\"Validation SSIM: {val_ssim:.4f}\")\n",
    "\n",
    "\n",
    "    # Final evaluation\n",
    "    print(\"\\nFinal Evaluation:\")\n",
    "    final_ssim = evaluate_model(student_model, val_lr_imgs, val_hr_imgs)\n",
    "    print(f\"Final SSIM Score: {final_ssim:.4f} ({final_ssim * 100:.2f}%)\")\n",
    "\n",
    "    # Show results\n",
    "    print(\"\\nSample Results:\")\n",
    "    for i in range(min(3, len(val_pairs))):\n",
    "        print(f\"\\nSample {i+1}:\")\n",
    "        # Use student_model for show_results\n",
    "        show_results(student_model, val_lr_imgs, val_hr_imgs, index=i)\n",
    "        print(\"MOS Rating (1-5): ______\")  # Manual evaluation\n",
    "\n",
    "    # Benchmark FPS\n",
    "    print(\"\\nFPS Benchmarking:\")\n",
    "    # Benchmark student_model\n",
    "    benchmark_fps(student_model, resolution=(1080, 1920))\n",
    "\n",
    "    # Save model\n",
    "    # Changed to use a Colab-compatible path in Google Drive\n",
    "    save_path = \"/content/drive/MyDrive/Student/student.pt\"\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    torch.save(student_model.state_dict(), save_path)\n",
    "    print(f\"\\nðŸŽ‰ Student model saved as '{save_path}'\")\n",
    "\n",
    "    return student_model, teacher_model, val_lr_imgs, val_hr_imgs # Return both models and validation data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c83ae1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Set your model path here\n",
    "model_path = \"C:/Users/LENOVO/Downloads/vedha/student_model.pt\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define Student Model (must match training definition)\n",
    "class StudentSharpeningModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.up = nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1)\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.Conv2d(32, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 3, 3, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.enc1(x)\n",
    "        x2 = self.enc2(x1)\n",
    "        up = self.up(x2)\n",
    "        up = up + x1  # Skip connection\n",
    "        out = self.dec(up)\n",
    "        return (out + 1) / 2  # Rescale from [-1, 1] to [0, 1]\n",
    "\n",
    "# Load model\n",
    "try:\n",
    "    loaded_model = StudentSharpeningModel().to(device)\n",
    "    loaded_model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    loaded_model.eval()\n",
    "    print(f\" Student model loaded successfully from:\\n   {model_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load model: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Load and preprocess input image\n",
    "def load_and_preprocess_image(image_path, target_size=(256, 256)):\n",
    "    try:\n",
    "        img = Image.open(image_path).convert(\"RGB\")\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(target_size, interpolation=Image.BICUBIC),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        return transform(img).unsqueeze(0).to(device)  # (1, 3, H, W)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image: {e}\")\n",
    "        return None\n",
    "\n",
    "# Save output\n",
    "def save_output_image(tensor, output_path=\"sharpened_output.png\"):\n",
    "    try:\n",
    "        img = transforms.ToPILImage()(tensor.squeeze(0).cpu())\n",
    "        img.save(output_path)\n",
    "        print(f\" Sharpened image saved to: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\" Error saving image: {e}\")\n",
    "\n",
    "# Main inference loop\n",
    "if __name__ == \"__main__\":\n",
    "    image_path = input(\" Enter path to the image you want to sharpen: \").strip()\n",
    "\n",
    "    if not os.path.exists(image_path):\n",
    "        print(\" File not found. Please check the path.\")\n",
    "        exit()\n",
    "\n",
    "    input_tensor = load_and_preprocess_image(image_path)\n",
    "\n",
    "    if input_tensor is not None:\n",
    "        print(\" Running inference...\")\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            output_tensor = loaded_model(input_tensor)\n",
    "        end_time = time.time()\n",
    "\n",
    "        print(f\" Inference time: {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "        # Save and show result\n",
    "        output_path = \"sharpened_output.png\"\n",
    "        save_output_image(output_tensor, output_path)\n",
    "\n",
    "        plt.imshow(transforms.ToPILImage()(output_tensor.squeeze(0).cpu()))\n",
    "        plt.title(\" Sharpened Output\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
